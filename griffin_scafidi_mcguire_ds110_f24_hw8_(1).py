# -*- coding: utf-8 -*-
"""Griffin_Scafidi_McGuire_DS110_F24_HW8 (1).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Y2BtFHtCHL326mbxN40iSB24xu-iskof

# HW8 (70 points total)

# Problem 1:   Forwarding the mail (8 points)

There's just one email address for *Handy Home Helpers*, but there are three kinds of things the business does, each of which is handled by a different associate.  Alice handles bathroom work - clogged toilets and drains, mold removal, leaks.  Bob handles HVAC work - air conditioning and heating, mostly.  Chun does appliance repair:  dishwashers, washers, driers.  They'd like to have a script that can forward emails sent to the main address to the right associate.

Write a function *forward(text, wv)* that reads the email *text* and returns the email address of the associate to whom the email should be forwarded ("alice@hhh.com", "bob@hhh.com", or "chun@hhh.com").  To do this, your code should create a vector for each associate that represents their interests overall, and return the email of the associate whose vector is closest in angle to the vector of the email.  Use *find_avg_word_vector()* and *find_cosine()* from lecture, and make use of the provided topic strings.  Break ties among the associates alphabetically, so for example Alice gets the mail if she's tied for closest.
"""

alice = 'bathrooom toilet clogged drain mold leaks'
bob = 'air-conditioning AC heating cooling vents air'
chun = 'appliances dishwashers washer drier repair'

# May need to !pip install gensim if working locally
import gensim
import gensim.downloader as api

wv = api.load('word2vec-google-news-300')

# TODO:  get relevant functions from lecture notes
import pandas as pd
import numpy as np

import nltk

from nltk.tokenize import word_tokenize
nltk.download('punkt')

nltk.download('punkt_tab')

def find_cosine(vec1, vec2):
  # Scale vectors to both have unit length
  unit_vec1 = vec1/np.linalg.norm(vec1)
  unit_vec2 = vec2/np.linalg.norm(vec2)
  # The dot product of unit vectors gives the cosine of their angle
  return np.dot(unit_vec1,unit_vec2)

def find_avg_vector(txt, embedding):
  words = word_tokenize(txt)
  vec_sum = None
  count = 0
  for word in words:
    if word in embedding:
      count += 1
      if vec_sum is not None:
        vec_sum += embedding[word]
      else:
        # The embeddings are read-only unless you copy them
        vec_sum = embedding[word].copy()
  if vec_sum is None:
    return pd.Series(np.zeros((300,)))  # Treat no word found in embedding as zero vector
  return pd.Series(vec_sum/count)

# TODO: forward()
vecs = [find_avg_vector(alice, wv), find_avg_vector(bob, wv), find_avg_vector(chun, wv)]
emails = ["alice@hhh.com", "bob@hhh.com", "chun@hhh.com"]
names = ['alice', 'bob', 'chun']

def forward(email_str, embedding):
  email_vec = find_avg_vector(email_str, wv)

  close_i = 0
  close_d = find_cosine(vecs[0], email_vec)

  for i in range(3):
      dis = find_cosine(vecs[i], email_vec)

      if dis > close_d:
        close_i = i
        close_d = dis

  return emails[close_i]

# Test1: Expect 'alice@hhh.com'
test1 = "Tried a plunger and no luck.  Help!"
forward(test1, wv)

#Test2: Expect 'bob@hhh.com'
test2 = "So hot - I think the thermostat is busted..."
forward(test2, wv)

#Test3: Expect 'chun@hhh.com'
test3 = "I need to fix my dishwasher - suds flood the kitchen every time I run it"
forward(test3, wv)

"""# Problem 2:  Price of Milk Interpolation (7 points)

The following y values represent the average price of a gallon of milk for each year. (Source: https://www.usinflationcalculator.com/inflation/milk-prices-adjusted-for-inflation/) Perform linear regression with scikit-learn's LinearRegression class.  Then plot the points alongside the linear fit.  And finally, make a prediction for 2023 with this linear model.
"""

import numpy as np
x = np.linspace(1995, 2021, 27)
y = [2.48, 2.62, 2.61, 2.70, 2.84,
     2.78, 2.88, 2.76, 2.76, 3.16, 3.19, 3.08, 3.50, 3.80, 3.11,
     3.26, 3.57, 3.49, 3.46, 3.69, 3.42, 3.20, 3.23, 2.90, 3.04,
     3.32, 3.55]

# TODO linear regression and plot
import matplotlib.pyplot as plt
import sklearn.linear_model as lm
from sklearn.linear_model import LinearRegression

linear_model = LinearRegression()
x = x.reshape(-1, 1)
linear_model.fit(x,y)
y_hat = linear_model.predict(x)

plt.plot(x, y, 'o')
plt.plot(x, y_hat, 'r')
plt.show()

# TODO: Prediction for 2023
Xval_predict = np.array([[2023]])
print(f'prediction for 2023: {linear_model.predict(Xval_predict)}')

"""# Problem 3:  Cumulative problems (35 points)

a (20 points):  Suppose we have a matrix where the $m$ rows represent different observations and the $n$ columns represent different features of the same example.  We also have an $(m-v)$-element 1D numpy array of labels for the training examples, 0 or 1, and a $v$ element numpy array with labels for the validation examples (which all come after the training examples in the feature matrix).  We'd like to train a bunch of decision trees, checking what happens when we train with all possible combinations of the following constructor parameters:  max_depth (2, 10, or None), min_samples_leaf (1 or 2), and max_features ('sqrt' or None).  The best tree is the one with the highest accuracy on the validation set.  Write a function find_best_params() that tries all combinations of the features and values described above.  Return the validation accuracy of the best tree, the max_depth of the best tree, the min_samples_leaf of the best tree, and the max_features of the best tree.
"""

import numpy as np

# Small example for basic testing - things should run
examples_example = np.array([[1, 2, 3], [-1, -2, -3], [3, 4, -2], [-6, 4, 1], [1, 2, -4], [1,1,1]])
train_labels_example = np.array([1, 0, 1, 0])
validation_labels_example = ([0, 1])

# Bigger example where params may matter
from sklearn.datasets import make_classification
big_examples, big_labels = make_classification(n_samples=200, n_features=20, n_classes=2, shuffle=True, random_state=0)
big_labels_train = big_labels[:150]
big_labels_valid = big_labels[150:]

# TODO: find_best_params()
from sklearn.tree import DecisionTreeClassifier

def find_best_params(examples, train_labels, valid_labels):
  examples_train = examples[:len(train_labels)]
  examples_valid = examples[len(train_labels):]

  max_depth_opt = [2, 10, None]
  min_samples_leaf_opt = [1, 2]
  max_features_opt = ['sqrt', None]

  best_score = 0
  best_params = {'max_depth': None, 'min_leafs': None, 'max_features': None}

  for depth in max_depth_opt:
    for leafs in min_samples_leaf_opt:
      for features in max_features_opt:

        dtree = DecisionTreeClassifier(criterion='entropy', random_state=0, max_depth=depth, min_samples_leaf=leafs, max_features=features)
        dtree.fit(examples_train, train_labels)

        score = dtree.score(examples_valid, valid_labels)

        if score > best_score:
          best_score = score
          best_params = {'max_depth': depth, 'min_leafs': leafs, 'max_features':features}

  print(f'SCORE: {best_score}\n')
  print(f'max_depth = {best_params["max_depth"]}')
  print(f'min_samples_leaf = {best_params["min_leafs"]}')
  print(f'max_features = {best_params["max_features"]}')

  return best_score, best_params

# With random_state = 0 passed to DecisionTreeClassifier,
# accuracy 0.5 - other parameter values depend on order of evaluation
find_best_params(examples_example, train_labels_example, validation_labels_example)

# With random_state = 0 passed to DecisionTreeClassifier,
# Accuracy of 0.9, max_depth varies, min_samples_leaf 1, max_features None
find_best_params(big_examples, big_labels_train, big_labels_valid)

"""b (15 points):  You are analyzing a long document that happens to mention who reports to who in an organization - for example, "Mary reports to Alice".  Write a function print_reports() that analyzes such a string, pulling out all such relationships.  (You can assume the document literally says "X reports to Y" with X and Y being the names.). Then print for each person the number of "direct reports" that person has, that is, the number of people reporting to them.  (See the tests for examples).  Return the total number of people in the organization."""

from http.client import FOUND
# TODO print_reports()
import re

def print_reports(strin):
  pattern = '(\w+) reports to (\w+)'
  matches = re.findall(pattern, strin)

  if matches == None:
    print('No Matches')
    return

  e_dict = {}
  boss_set = set([boss for e, boss in matches])

  for empl, boss in matches:
    if boss not in e_dict:
      e_dict[boss] = 0
    e_dict[boss] += 1
    if empl not in e_dict:
      e_dict[empl] = 0

  for person in e_dict:
    print(f'{person} has {e_dict[person]} employees')

  return len(e_dict)

text = "Mary reports to Sally, Sally reports to Bob, Bob reports to Alice, \
        but also Yilan reports to Sally, and Medhavi reports to Alice"
# Expect Sally 2, Mary 0, Bob 1, Yilan 0, Medhavi 0, Alice 2
# (in no particular order) and return 6
print_reports(text)

# Don't crash on this
text = "This document has no information"
print_reports(text)

# Expect Alice 4, all others 0, total people 5
text = "Bob reports to Alice and Dominique reports to Alice, \
        not to mention Jason reports to Alice and Clive reports to Alice"
print_reports(text)

"""# Problem 4:  Miniproject (20 points + up to 15 points extra credit)

This problem has an open-ended component.  You can get full credit by doing the problem set with the *suggested* data set (spotify.csv), but you can get **extra credit** by doing it with a different dataset that you have found on the Internet (for example, on Kaggle.com) and/or doing more analysis than required.  You also have the opportunity to get **additional extra credit applied to your final exam grade** if you are selected to give a lightning talk on Dec 6 about what you found with this project.  The lightning talk is only possible if you chose your own dataset.

The suggested dataset, guaranteed to have minimal data cleaning but not eligible for extra credit, is the file spotify.csv, which originally came from https://www.kaggle.com/datasets/maharshipandya/-spotify-tracks-dataset but which you can now find on Blackboard where you found this assignment.  (For a description of what its features mean, see https://developer.spotify.com/documentation/web-api/reference/get-audio-features.)  We'll assume in the instructions that if you're using that dataset, you're trying to predict genre from the other numerical features.

If you choose your own dataset, pick one where you think it would be interesting but feasible to predict some variable in the dataset from the others.  If it needs a lot of "cleaning" to be usable, you will get extra credit, but you could also consider looking at a few datasets and picking one that seems somewhat close to directly usable.  (You do *not* need high accuracy in your classifier to get full or extra credit; problems like predicting stock prices from their histories are inherently harder than classifying country from latitude and longitude, for example.)

a 5 points + 7 points EC) Load the dataset as a DataFrame and prepare it for machine learning.  In the spotify.csv case, we suggest using a sklearn.preprocessing.LabelEncoder to turn the target column into numerical classes; see examples in the documentation (https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html).  You should also fix any columns that look basically numerical, but for some reason were processed as strings instead; but the suggested dataset doesn't have any of these.

If you are cleaning a novel dataset, you may get extra credit here if it takes more work than the spotify dataset to clean.

Suggested dataset EC:  For the suggested dataset, you will get better results if you limit the target genres to four easily distinguished categories, such as 'acoustic', 'dance', 'grunge', and 'show-tunes'.  You can use .unique() to see all the categories available in this column.  Choosing your own 4 categories from unique() is worth 1 point of extra credit.  Limiting the classes like this is otherwise optional.
"""

!pip install ucimlrepo

from ucimlrepo import fetch_ucirepo

# fetch dataset
census_income = fetch_ucirepo(id=20)

# data (as pandas dataframes)
X = census_income.data.features
y = census_income.data.targets

# metadata
print(census_income.metadata)

# variable information
print(census_income.variables)

X.head()

X.dtypes

def get_options(data_frame, column_name):
  seen = []
  for val in data_frame[column_name]:
    if val not in seen:
      seen.append(val)
  return seen

def str_to_num(data_frame, column_name):
  key = {}
  count = 0
  for tipe in get_options(data_frame, column_name):
    data_frame[column_name].replace(tipe, count, inplace=True)
    key[tipe] = count
    count +=1

  print(key)
  return data_frame

for col_name in X.columns:
  if X[col_name].dtype == 'object':
    str_to_num(X, col_name)

y.head()

get_options(y, 'income')

y['income'].replace('<=50K.', '<=50K', inplace=True)
y['income'].replace('>50K.', '>50K', inplace=True)

str_to_num(y, 'income')

"""b, 7 points) Try predicting your target variable using a RandomForestClassifier from scikit-learn, with all the other numerical features in the dataset as your features.  You can create a dataframe that includes just your numeric features with df.select_dtypes(include='number'), and drop your target (to-be-predicted) column from your features if you need to with df = df.drop(columns=['target']).  (The suggested dataset should also drop the first 'Unnamed' column - that row number predicts the genre number pretty well in that dataset!)  Use a train-test split with 10% of the data in the test set, and evaluate the accuracy on the test set."""

from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier

data_train, data_test, labels_train, labels_test = train_test_split(X, y)
clf = RandomForestClassifier()
clf.fit(data_train, labels_train)
clf.score(data_test, labels_test)

"""c, 4 points) Choose one argument to RandomForestClassifier besides n_estimators that you vary to try to improve your classifier's accuracy.  (See documentation here:  https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html)  Train three different classifiers with different values for this parameter, counting the one you already trained."""

clf2 = RandomForestClassifier(criterion='entropy')
clf2.fit(data_train, labels_train)
clf2.score(data_test, labels_test)

clf3 = RandomForestClassifier(criterion='log_loss')
clf3.fit(data_train, labels_train)
clf3.score(data_test, labels_test)

"""d, 2 points) Use the *_feature_importances* attribute of the RandomForestClassifier to find the relative importances of all your features in your best model."""

features = clf.feature_importances_

print('Feature Importances for Forest\n')

for idx in range(len(features)):
  print(f'  {X.columns[idx]}: {features[idx]}\n')

"""e, up to 8 points EC) In this step, perform some additional analysis of your choice on your dataset, such as looking at correlations, performing statistical tests, or training a different machine learning classifier or regression.  You could also plot data for credit, using scatter plots, bar charts, or other visualizations.  Choose your methods with an eye toward being interesting for step (f).  This step is extra credit."""

import matplotlib.pyplot as plt
import seaborn as sns

C = X
C['income'] = y

corr = C.corr()

plt.figure(figsize=(14,10))
sns.heatmap(corr, annot=True)
plt.title('Correlation')
plt.show()

for col in X.columns:
    X[col] = X[col] / (max(X[col]))

X.head()

from sklearn.neighbors import KNeighborsClassifier

nbrs = KNeighborsClassifier(n_neighbors=29).fit(data_train, labels_train)
nbrs.score(data_test, labels_test)

"""f, 2 points plus shot at lightning talk) Look over your findings from parts (a-e) and summarize anything interesting you learned about the data from doing this study.  The students with the best answers to this question (who also chose to analyze novel datasets) may be selected to give lightning talks for additional extra credit.

**TODO**

I was suprised that the decision tree classifier worked better than K nearest neighbors on this dataset, but because I imagined there would be lots of people in similar situations within the same income 'bracket'. Instead the only 'heavily' correlated feature to ones income was marital status, and even this wasn't correlated that much. This is also cool because we just talked about this in my 109 lecture and to see it confirmed with data off the internet is validating. Finally, I think a lot of people make very big generalizations and assumptions about others based on their race, gender, age, job, ect... But (at least going off this dataset), these do not always decide someones fate (to make over or under 50k per year). Computers can make those assumptions sometimes with cool math and stuff and yeild 85% accuracy, but I sure couldn't from this data set. #dont judge a book by its cover!
"""